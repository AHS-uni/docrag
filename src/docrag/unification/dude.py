import json
from pathlib import Path

from docrag.schema import (
    Answer,
    AnswerFormat,
    AnswerType,
    Document,
    Evidence,
    EvidenceSource,
    Question,
    QuestionType,
    UnifiedEntry,
)
from docrag.schema.raw import DUDERawEntry
from docrag.schema.utils import tag_inferred, tag_low_quality, tag_missing

from .unifier import Unifier
from .registry import register_unifier
from .exceptions import UnificationError

__all__ = ["DUDEUnifier"]


@register_unifier("dude")
class DUDEUnifier(Unifier[DUDERawEntry]):
    """
    Unifier for the DUDE competition dataset.
    """

    def _discover_raw_qas(self) -> list[Path]:
        # All of the DUDE files are plain JSON under raw_qas/
        return sorted(self.raw_qas_directory.glob("*.json"))

    def _load_raw_qas(self, path: Path) -> list[DUDERawEntry]:
        # Entries are in a top level 'data' array
        payload = json.loads(path.read_text(encoding="utf-8"))
        return [DUDERawEntry.model_validate(item) for item in payload["data"]]

    def _build_entry(
        self,
        raw: DUDERawEntry,
        document: Document,
        question: Question,
        evidence: Evidence,
        answer: Answer,
    ) -> UnifiedEntry:
        entry = UnifiedEntry(
            id=f"{raw.doc_id}-{raw.question_id}",
            document=document,
            question=question,
            evidence=evidence,
            answer=answer,
        )

        if raw.data_split.strip().lower() == "test":
            entry.tags.append(
                tag_missing(
                    "evidence",
                    "DUDE is a competition dataset. No evidence provided in test split.",
                )
            )
            entry.tags.append(
                tag_missing(
                    "answer",
                    "DUDE is a competition dataset. No answer provided in test split.",
                )
            )

        return entry

    def _build_document(self, raw):
        count_pages = len(self._corpus_documents.get(raw.doc_id, []))

        if count_pages == 0:
            if self.test_mode:
                self.logger.warning(
                    "Document not found in corpus: doc_id=%s", raw.doc_id
                )
                self._problematic["documents"].append({
                    "document_id": raw.doc_id,
                    "reason": "Document missing in corpus records.",
                })
                return None
            else:
                raise UnificationError(
                    f"Document '{raw.doc_id}' not found in corpus."
                    f"for question_id={raw.question_id}."
                )

        document = Document(id=raw.doc_id, count_pages=count_pages)
        document.tags.append(tag_missing("type", "DUDE does not provide document types."))
        return document

    def _build_question(self, raw):
        question = Question(id=raw.question_id, text=raw.question)
        if raw.data_split == "test":
            question.tags.append(tag_missing(
                "type",
                "Question type is not available in the test split. DUDE considers question type to be a part of the answer.",
            ))
            return question

        assert raw.answer_type, "Expected non-empty answer type for non-test questions"

        question.type = self._map_question_type(raw.answer_type.strip().lower())
        question.tags.append(tag_low_quality(
            "type",
            "Question type classification in DUDE is limited. Only provides four broad categories.",
        ))

        return question

    def _build_evidence(self, raw):
        evidence = Evidence()

        if raw.data_split.strip().lower() == "test":
            return evidence

        assert raw.answer_type, "Expected non-empty answer type for non-test questions"

        if raw.answer_type.strip().lower() == "not-answerable":
            evidence.sources = [EvidenceSource.NONE]
            return evidence

        document_pages = self._corpus_documents.get(raw.doc_id, [])
        if not raw.answers_page_bounding_boxes:
            evidence.pages = document_pages
            evidence.tags.append(tag_inferred(
                "pages",
                "No evidence pages provided for this question. Using all document pages."
            ))
            evidence.tags.append(tag_missing("sources", "DUDE does not provide evidence sources."))
            return evidence

        pages = {b.page for grp in raw.answers_page_bounding_boxes for b in grp}
        missing = [p for p in pages if p not in document_pages]
        if missing:
            if self.test_mode:
                valid_pages = sorted(p for p in pages if p in document_pages)
                evidence.pages = valid_pages or [0]
                self.logger.warning(
                    "Extracted pages %s not in corpus for doc_id=%s. Using %s instead.",
                    missing, raw.doc_id, evidence.pages
                )
                self._problematic["questions"].append({
                    "question_id": raw.question_id,
                    "reason": f"Extracted pages {missing} not found in corpus.",
                })
            else:
                raise UnificationError(
                    f"Pages {missing} extracted from bounding boxes do not exist in corpus for "
                    f"doc_id={raw.doc_id}, question_id={raw.question_id}."
                )

        evidence.pages = sorted(pages)
        evidence.tags.append(tag_missing("sources", "DUDE does not provide evidence sources."))
        return evidence

    def _build_answer(self, raw):
        if raw.data_split.strip().lower() == "test":
            return Answer()
        if raw.answer_type == "not-answerable":
            return Answer(type=AnswerType.NOT_ANSWERABLE)

        assert raw.answers, "Expected non-empty answers list for answerable non-test questions"

        answer = Answer()
        if len(raw.answers) > 1:
            answer.variants = [self._handle_list_answers(raw.answers)]
            answer.format = AnswerFormat.LIST
        else:
            single = str(raw.answers[0])
            answer.variants = [single] + (raw.answers_variants or [])

        answer.type = AnswerType.ANSWERABLE
        answer.tags.append(tag_missing("format", "DUDE does not provide answer formats."))
        answer.tags.append(tag_missing("rationale", "DUDE does not provide rationale for answers."))
        return answer

    def _map_question_type(self, answer_type: str) -> QuestionType:
        """
        Map the raw answer_type string to our QuestionType enum.
        """
        mapping: dict[str, QuestionType] = {
            "extractive": QuestionType.EXTRACTIVE,
            "list/extractive": QuestionType.EXTRACTIVE,
            "abstractive": QuestionType.ABSTRACTIVE,
            "list/abstractive": QuestionType.ABSTRACTIVE,
        }
        return mapping.get(answer_type, QuestionType.OTHER)

    def _handle_list_answers(self, answers: list[str]) -> str:
        """
        Convert a list of answer items into a single string representation,
        e.g. ["foo", "bar"] â†’ "['foo', 'bar']".
        """
        item_strings = [f"'{item}'" for item in answers]
        return "[" + ", ".join(item_strings) + "]"
